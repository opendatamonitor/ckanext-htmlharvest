from bs4 import BeautifulSoup
import urllib2
import requests
from xml.dom.minidom import parseString
import re
import difflib
import json
import logging
import SaveLabels
import pymongo
import harvester_final
import AddNoLabelToJson
import AddLabelToJson
import AddLinkToJson
import hashlib
import AddToCkan
import time

def ProcedureWithNext(soup1,dataset_keyword,dataset_keyword1,mainurl,text_file,ckanjason,commands,ckannotes,ckanlicense,ckanresource,ckantags,ckancontactpoint,ckanpublisher,j,label,k,a_link,
			  CheckType,links,type1,jason,db1,endpoint,url,i,afterid,step,ckantitle,ckanExtrasUpdateDate,ckanExtrasCategory,ckanExtrasFrequency,ckanExtrasLanguage,ckanExtrasVersion,ckanMaintainer,
			  ckanExtrasDate,ckangeographiccoverage,ckantemporalcoverage):
  soup1=endpoint
  xtras=[]
  xtras1=[]
  LinkCounter=0
  LastLinkCounter=0
	


  while endpoint in soup1:
	  
	  try:
		  url1=str(url)+str(i)+str(afterid)
		 # start=time.time()
		  print(url1)
		  r  = requests.get(url1)
		  
		  data = r.text
		  soup = BeautifulSoup(data)
		  soup1=str(soup)
		 
		  
		  #text_file.write(str(theo))
		  for link in soup.find_all('a'):
							  
			  try:
				  ahref=str(link.get('href')).encode('utf-8')
				  
			  except UnicodeEncodeError: 
				#edw den upirxe to print
				text_file.write('  UnicodeError in link  ')
				
		  
			  if (dataset_keyword in ahref )or((dataset_keyword1 in ahref )):
				  LastLinkCounter=1
				  text_file.write(str(mainurl)+"  "+str(ahref))
				  
				 # if ahref.find(mainurl) == -1:
					
				# url2=ahref
				  if mainurl not in ahref:
					url2=mainurl+'/'+ahref
				  else: url2=ahref
				  text_file.write('\n'+'try to open:  '+str(url2)+'\n')
				  #url2=ahref gia to http://digitaliser.dk/               <--auto
				  print(str(url2))
				  text_file.write('\n')
				  text_file.write("ahref: "+str(url2))
				  
				  ckanjason=ckanjason+"'url':"+"'"+str(url2)+"'"+","
				  text_file.write('\n')
				  text_file.write('\n')
				  r2  = requests.get(url2)
				  data2 = r2.text
				  
			  
				  soup2 = BeautifulSoup(data2)
				  
				  counterl=0
				  
			  #--#--- for non label data
				  ckanjason,xtras1=AddNoLabelToJson.AddToJson(commands,counterl,ckannotes,ckanlicense,ckanresource,ckantags,ckancontactpoint,ckanpublisher,soup2,text_file,ckanjason,ckantitle,ckanExtrasUpdateDate
				  ,ckanExtrasCategory,ckanExtrasFrequency,ckanExtrasLanguage,ckanExtrasVersion,ckanMaintainer,ckanExtrasDate,ckangeographiccoverage,ckantemporalcoverage)
				  
				  counterl=0
				 
				  title=soup2.find_all('title')
				  text_file.write(str(title))
				  title2=str(title)
				  text_file.write(str("Title: "+title2.replace('[<title>','').replace(']','').replace("'","").replace('"','').replace('</title>','').strip()))
				  text_file.write('\n')
				  if ckantitle=="":
					ckanjason=ckanjason+"'title':"+'"'+str(title2.replace('[<title>','').replace(']','').replace('|','').replace('</title>','').replace("'","").replace('"','').strip())+'"'+","
					ckanjason=ckanjason+"'name':"+"'"+str(hashlib.md5(title2.replace('[<title>','').replace(']','').replace('|','').replace('</title>','')).hexdigest())+"'"+","
				  
				  
			  #--- for label data	

				  soup3=soup2.findAll(text=True)
				  
				  while j<len(label):				
					  while k<len(soup3):
						  if label[j] in soup3[k]:

							  label[j]=soup3[k]
						  k=k+1
					  k=0
					  j=j+1
				  j=0
			  
			  #for label data
				  
				  ckanjason=AddLabelToJson.AddToJson(soup3,label,j,ckannotes,ckanlicense,ckanresource,ckantags,ckancontactpoint,ckanpublisher,text_file,ckanjason,a_link,CheckType,ckanExtrasUpdateDate,ckanExtrasCategory
				  ,ckanExtrasFrequency,ckanExtrasLanguage,ckanExtrasVersion,ckanMaintainer,ckanExtrasDate,xtras1,ckangeographiccoverage,ckantemporalcoverage)
				  
				  j=0
			  

				  #--- links searching
			  
				  while j<len(links):				
					  while k<len(soup3):
						  if links[j] in soup3[k]:

							  links[j]=soup3[k]
						  k=k+1
					  k=0
					  j=j+1
				  j=0
			  
			  # Add links to json
			  
				  ckanjason=AddLinkToJson.AddToJson(links,soup3,ckannotes,ckanlicense,ckanresource,ckantags,ckancontactpoint,ckanpublisher,text_file,ckanjason,j,type1,CheckType)
				  
				  j=0

				  #-- store twn metadata sti vasi

				  #text_file.write('jason: '+str(jason.rstrip(','))+'}')
				  text_file.write('\n'+'\n'+'ckanjason: '+str(ckanjason.rstrip(','))+'}')
				  json2=jason.rstrip(',')+'}'
				  ckanjson3=ckanjason.rstrip(',')+'}'
				  
				  ckanjsonWithoutTags=ckanjson3.replace(ckanjson3[ckanjson3.find("'tags':[{")-1:ckanjson3.find("}]")+2],'')
				  
				
				  #json1='json='+str(json2)
				  
				  ckanjson1='ckanjson2='+str(ckanjson3)
				  
				  ckanjsonWithoutTags1='ckanjsonWithoutTags2='+str(ckanjsonWithoutTags)
				  #exec(json1)
				  try:
					
					exec(ckanjson1)					#print(str(ckanjson2))
					try:
					  text_file.write("\n"+"\n"+">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"+str(ckanjson2)+"\n"+"\n"+"\n")
					  AddToCkan.AddtoCkan(ckanjson2)
					  db1.save(ckanjson2)
					  time.sleep(0)
					  
				  #jason="{"
					except urllib2.HTTPError, e:
					  text_file.write("DB_ERROR")
					  
					  try:
						# an apotyxei me ta tags add to ckan without tags!!
						
						exec(ckanjsonWithoutTags1)
						
						try:
						  
						  AddToCkan.AddtoCkan(ckanjsonWithoutTags2)
				  #jason="{"
						except urllib2.HTTPError, e:
						  j+=1
						  text_file.write("DB_FATAL_ERROR")
				
					  except SyntaxError:
						  j+=1
						  text_file.write("SYNTAX_ERROR1")
				  except SyntaxError:
					#itan k edw 
					k+=1
					text_file.write("SYNTAX_ERROR")
				  
				  ckanjason="{"
				  

				  
	  except urllib2.HTTPError, e:
		  #j+=1
		  #i=i-step
		  text_file.write("404 ERROR")
		  #jason="{"
		  print('i: ='+str(+i))
	  if LastLinkCounter==1:
		i=i+step
		LastLinkCounter=0
	  else:
		if LinkCounter<=3:
		  LastLinkCounter=0
		  i=i+step
		  LinkCounter+=1
		else: break